{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "lib_dir = \"g:/My Drive/Storage/Github/hyuckjinkim\"\n",
    "sys.path.append(lib_dir)\n",
    "\n",
    "from lib.python.graph import MatplotlibFontManager\n",
    "fm = MatplotlibFontManager()\n",
    "fm.set_korean_font(check=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic gate : 51p ~ 59p\n",
    "def gate(values, gate_type):\n",
    "    values = np.array(values, dtype=np.float64)\n",
    "\n",
    "    gate_type = gate_type.upper()\n",
    "    weights = np.ones_like(values) / len(values)\n",
    "\n",
    "    assert all(v in [0,1] for v in values), \"Each value must be either 0 or 1.\"\n",
    "    assert gate_type in ['AND','OR','NAND','XOR'], \"gate_type must be one of ['AND','OR','NAND','XOR'].\"\n",
    "    \n",
    "    # AND, OR, XOR 그래프 참조 : https://blog.naver.com/hyemin8670/222428861609\n",
    "    if gate_type=='XOR':\n",
    "        # 재귀함수 생성\n",
    "        # XOR(x) = AND(NAND(x),OR(x))\n",
    "        nand_output = gate(values, 'nand')\n",
    "        or_output = gate(values, 'or')\n",
    "        xor_output = gate([nand_output, or_output], 'and')\n",
    "        return xor_output\n",
    "\n",
    "    else:\n",
    "        offset = 0.2\n",
    "        bias_dict = {\n",
    "            'AND' : - sum(weights[1:]) - offset,\n",
    "            'OR' :  - weights[0] + offset,\n",
    "            'NAND' : - sum(weights[1:]) - offset, # AND gate 활용\n",
    "        }\n",
    "        bias = bias_dict.get(gate_type)\n",
    "        z = values @ weights + bias\n",
    "\n",
    "        if gate_type in ['AND','OR']:\n",
    "            gate_output = 1 if z>=0 else 0\n",
    "        elif gate_type in ['NAND']:\n",
    "            gate_output = 1 if z<0 else 0 # not AND\n",
    "        return gate_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 Gate에서의 진리표 테스트\n",
    "values_comb = [[x,y] for x in [0,1] for y in [0,1]]\n",
    "# values_comb = [[x,y,z] for x in [0,1] for y in [0,1] for z in [0,1]]\n",
    "gate_types = ['AND','OR','NAND','XOR']\n",
    "\n",
    "for gate_type in gate_types:\n",
    "    print(f'\\n## {gate_type=}')\n",
    "    for values in values_comb:\n",
    "        print(f'{values=}: output={gate(values, gate_type)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid & Arctangent : 73p\n",
    "def sigmoid(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = np.linspace(-10,10,num=100)\n",
    "y1 = sigmoid(x)\n",
    "y2 = np.arctan(x)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(x,y1,'-')\n",
    "plt.plot(x,y2,'-')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axhline(0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.axvline(0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.legend(['sigmoid','arctan'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax : 94p\n",
    "def softmax(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "\n",
    "    z = x - np.max(x, axis=-1, keepdims=True) # overflow 방지\n",
    "    numerator = np.exp(z)\n",
    "    denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "    softmax = numerator / denominator\n",
    "    \n",
    "    return softmax\n",
    "\n",
    "x = [[199,200,200], [1,2,3]]\n",
    "print(softmax(x).round(4))\n",
    "print(softmax(x).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy : 115p\n",
    "\n",
    "# y_true와 y_pred의 모든 요소를 고려하여 계산하는 방법\n",
    "def cross_entropy_error(y_true, y_pred):\n",
    "    delta = 1e-7\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.float64)\n",
    "    y_pred = np.array(y_pred, dtype=np.float64)\n",
    "    \n",
    "    cee = -np.sum(y_true * np.log(y_pred+delta))\n",
    "    return cee\n",
    "\n",
    "# y_true가 원-핫 인코딩일 경우, 정답에 해당하는 클래스의 예측 확률에만 집중해서 계산하는 방법\n",
    "# (1) 계산 효율성: 정답이 아닌 클래스에 대해서도 불필요한 곱셈과 로그 계산을 줄임\n",
    "# (2) 정확성: 정답 클래스에 해당하는 예측 확률만을 사용해 손실을 계산하므로, 실제로 필요한 정보를 정확하게 추출\n",
    "def cross_entropy_error(y_true, y_pred):\n",
    "    delta = 1e-7\n",
    "\n",
    "    y_true = np.array(y_true, dtype=np.float64)\n",
    "    y_pred = np.array(y_pred, dtype=np.float64)\n",
    "\n",
    "    if y_pred.ndim == 1:\n",
    "        y_true = y_true.reshape(1, y_true.size)\n",
    "        y_pred = y_pred.reshape(1, y_pred.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if y_true.size == y_pred.size:\n",
    "        y_true = y_true.argmax(axis=1)\n",
    "    else:\n",
    "        y_true = y_true.astype('uint')\n",
    "\n",
    "    # 정답 클래스에 해당하는 예측 확률만을 사용해 batch size 내 평균손실을 계산\n",
    "    batch_size = y_pred.shape[0]\n",
    "    cee = -np.mean(np.log(y_pred[np.arange(batch_size), y_true] + delta))\n",
    "\n",
    "    return cee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [0,0,1,0,0,0,0,0,0,0]\n",
    "y_pred = [0.1,0.05,0.6,0.0,0.05,0.1,0.0,0.1,0.0,0.0]\n",
    "cross_entropy_error(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치미분값 : 123p\n",
    "def numerical_diff(f,x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "# 접선기울기값 : 125p\n",
    "def tangent_line(f, x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    d = numerical_diff(f, x) # f'(x) : 기울기\n",
    "    # 접선의 방정식을 f'(x0) = d*x0 + y0로 표현하면, x0값이 주어질 때 이에 해당하는 y축절편의 값인 y0를 모름\n",
    "    # 이 값을 유도하기위해, 기존 함수인 f(x0)=f'(x0)를 사용한다. (x0에서의 접선이므로, f(x0)값과 f'(x0)값은 같으므로)\n",
    "    # 즉, y0 = f'(x0) - d*x0 = f(x0) - d*x0가 된다.\n",
    "    y = f(x) - d*x\n",
    "\n",
    "    # f'(x) = 기울기 * x값 + y절편값\n",
    "    return lambda t: d*t + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    #return x**2\n",
    "    return (x-2)*(x-1)*(x+3)\n",
    "\n",
    "x = np.arange(-5.0, 5.0, 0.1)\n",
    "\n",
    "x0 = 2\n",
    "tf = tangent_line(f, x0)\n",
    "y1 = f(x)\n",
    "y2 = tf(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y1, color='black')\n",
    "plt.plot(x, y2, color='green', alpha=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.grid()\n",
    "plt.legend(['f(x)',f'x={x0}에서의 접선방정식'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivative : 127p\n",
    "def _numerical_gradient_no_batch(f, x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x, dtype=np.float64)\n",
    "\n",
    "    for idx in np.ndindex(x.shape):\n",
    "        # 해당 idx에만 h를 더해주거나 빼주도록 h vector를 생성\n",
    "        h_vector = np.zeros_like(x)\n",
    "        h_vector[idx] = h\n",
    "\n",
    "        grad[idx] = (f(x+h_vector) - f(x-h_vector)) / (2*h)\n",
    "\n",
    "    return grad\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    X = np.array(X, dtype=np.float64)\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        return grad\n",
    "\n",
    "def f(x):\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "    if x.ndim == 1:\n",
    "        return np.sum(x**2)\n",
    "    else:\n",
    "        return np.sum(x**2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [[3,4],[0,2],[3,0]]:\n",
    "    x = np.array(x)\n",
    "    print(numerical_gradient(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다른 방식의 수치적 기울기 계산 함수\n",
    "def numerical_gradient_2(f, x):\n",
    "    # 이 부분이 추가되지 않으면 grad 결과값의 오차가 커짐\n",
    "    x = np.array(x, dtype=np.float64)\n",
    "\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)  # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)  # f(x-h)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2 * h)\n",
    "        x[idx] = tmp_val  # 원래 값 복원\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in [[3,4],[0,2],[3,0]]:\n",
    "    x = np.array(x)\n",
    "    print(numerical_gradient_2(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient plot : 129p\n",
    "def gradient_plot(f, minval, maxval):\n",
    "    x1 = x2 = np.linspace(minval, maxval, num=20)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    Y = f([X1,X2])\n",
    "\n",
    "    # calculate the gradient\n",
    "    grad = numerical_gradient(f, [X1.flatten(), X2.flatten()])\n",
    "\n",
    "    # visualization\n",
    "    fig = plt.figure(figsize=(15,7))\n",
    "\n",
    "    # (1) 2d gradient vector field\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax1.quiver(X1, X2, -grad[0], -grad[1], angles=\"xy\", color=\"#666666\")#,headwidth=10,scale=40,color=\"#444444\")\n",
    "    ax1.set_xlim([minval, maxval])\n",
    "    ax1.set_ylim([minval, maxval])\n",
    "    ax1.set_xlabel('x1')\n",
    "    ax1.set_ylabel('x2')\n",
    "    ax1.grid()\n",
    "    ax1.set_title('2D Gradient Vector Field')\n",
    "\n",
    "    # (2) 3d plot\n",
    "    ax2 = fig.add_subplot(122, projection='3d')\n",
    "    ax2.plot_surface(X1, X2, Y, cmap='viridis')\n",
    "    ax2.set_xlabel('x1')\n",
    "    ax2.set_ylabel('x2')\n",
    "    ax2.set_zlabel('y')\n",
    "    ax2.grid()\n",
    "    ax2.set_title('3D Plot')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_plot(lambda t: sum(np.array(t)**2), -2, 2)\n",
    "gradient_plot(lambda t: sum(np.array(t)**3), -100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent : 131p\n",
    "def gradient_descent(f, init, lr=0.01, step_num=100):\n",
    "    x = np.array(init, dtype=np.float64)\n",
    "    x_history = []\n",
    "\n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x, np.array(x_history)\n",
    "\n",
    "# gradient descent plot : 132p\n",
    "def gradient_descent_plot(f, init, lr, step_num, minval, maxval):\n",
    "    x, x_history = gradient_descent(f, init, lr=lr, step_num=step_num)\n",
    "\n",
    "    offset = 1.2\n",
    "    x1 = x2 = np.linspace(minval*offset, maxval*offset, num=20)\n",
    "    X1, X2 = np.meshgrid(x1, x2)\n",
    "    # [X1,X2]로 하게되면 2개의 batch로 인식 할 수 있으므로 [[X1,X2]]로 묶어주고, squeeze를 통해 차원이 1인 축을 제거\n",
    "    Y = f([[X1,X2]]).squeeze()\n",
    "\n",
    "    # visualization\n",
    "    plt.figure(figsize=(7,7))\n",
    "\n",
    "    # (1) contour\n",
    "    plt.contour(X1, X2, Y, levels=max(abs(minval),abs(maxval)), colors='gray', linestyles='dashed')\n",
    "\n",
    "    # (2) gradient descent moving\n",
    "    for i in range(len(x_history)-1):\n",
    "        val_dict = {\n",
    "            'X' : x_history[i, 0],\n",
    "            'Y' : x_history[i, 1],\n",
    "            'U' : x_history[i+1, 0] - x_history[i, 0],\n",
    "            'V' : x_history[i+1, 1] - x_history[i, 1],\n",
    "        }\n",
    "        plt.quiver(*list(val_dict.values()), angles='xy', scale_units='xy', scale=1.1, color='blue', width=0.003, alpha=0.5)\n",
    "        plt.annotate(str(i+1), [val_dict['X']-0.3, val_dict['Y']], color='blue', alpha=0.5)\n",
    "\n",
    "    # base line\n",
    "    plt.axvline(0, color='black', linestyle='-', alpha=0.8)\n",
    "    plt.axhline(0, color='black', linestyle='-', alpha=0.8)\n",
    "    plt.plot(x_history[:,0], x_history[:,1], 'ob', markersize=3)\n",
    "\n",
    "    plt.xlim(minval, maxval)\n",
    "    plt.ylim(minval, maxval)\n",
    "    plt.xlabel(\"x1\")\n",
    "    plt.ylabel(\"x2\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_plot(f, init=[-3,4], lr=0.1, step_num=20, minval=-6, maxval=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple network : 134p\n",
    "# -> loss() 함수에 w를 넣지 않으면 lambda w: net.loss(x,t)에서 w 매개변수가 없어 기울기를 계산할 수 없음\n",
    "class SimpleNet:\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size    # 입력된 데이터의 개수\n",
    "        self.output_size = output_size  # true label의 nunique\n",
    "\n",
    "        np.random.seed(42)\n",
    "        self.weight = np.random.randn(input_size, output_size)\n",
    "        self.bias = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x, weight=None, bias=None):\n",
    "        if weight is None:\n",
    "            weight = self.weight\n",
    "        if bias is None:\n",
    "            bias = self.bias\n",
    "        z = x @ weight + bias\n",
    "        return z\n",
    "\n",
    "    def loss(self, x, y_true, weight=None, bias=None):\n",
    "        if weight is None:\n",
    "            weight = self.weight\n",
    "        if bias is None:\n",
    "            bias = self.bias\n",
    "        z = self.predict(x, weight, bias)\n",
    "        y_pred = softmax(z)\n",
    "        loss = cross_entropy_error(y_true=y_true, y_pred=y_pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0.6,0.9]\n",
    "t = [1,0,0] # true label: 0,1,2 중 2이라는 뜻의 one-hot encoding\n",
    "\n",
    "# x = [0.5, 0.4, 0.8]\n",
    "# t = [0,1]\n",
    "\n",
    "net = SimpleNet(input_size=len(x), output_size=len(t))\n",
    "print('> Weights:') # weight matrix\n",
    "print(net.weight,'\\n')\n",
    "\n",
    "z = net.predict(x) # z\n",
    "print(f'> z: {z} \\n')\n",
    "\n",
    "y_pred = softmax(z)\n",
    "print(f'> Predicted Probability: {y_pred} \\n')\n",
    "\n",
    "prediction = np.argmax(z) # prediction label\n",
    "print(f'> Predicted Label: {prediction} \\n')\n",
    "\n",
    "loss = net.loss(x, t)\n",
    "print(f'> Cross Entropy Loss: {loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dw (weight의 기울기)\n",
    "\n",
    "# (1) 수치적 기울기 계산\n",
    "# -> loss() 함수에 w를 넣지 않으면 lambda w: net.loss(x,t)에서 w 매개변수가 없어 기울기를 계산할 수 없음\n",
    "# -> 따라서, w를 매개변수로 넣어줘야함\n",
    "f = lambda w: net.loss(x, t, w, None)\n",
    "dW1 = numerical_gradient(f, net.weight[np.newaxis, :, :])\n",
    "print(dW1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (2) 해석적 기울기 계산\n",
    "# 참조 : https://davidbieber.com/snippets/2020-12-12-derivative-of-softmax-and-the-softmax-cross-entropy-loss/\n",
    "def analytical_gradient(x, y_true, y_pred):\n",
    "    return np.outer(x, (y_pred - y_true))\n",
    "\n",
    "# 해석적 기울기 계산\n",
    "dW2 = analytical_gradient(x, t, y_pred)\n",
    "print(dW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dW1-dW2).round(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2층 신경망 구현 : 137p\n",
    "from tqdm import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return np.where(x>0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "class MultiLayerNet:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation=None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "\n",
    "        self.node_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.n_layers = len(self.node_sizes)-1\n",
    "\n",
    "        if activation in ['sigmoid',None]:\n",
    "            self.activation = sigmoid\n",
    "            self.activation_derivative = sigmoid_derivative\n",
    "        elif activation=='relu':\n",
    "            self.activation = relu\n",
    "            self.activation_derivative = relu_derivative\n",
    "\n",
    "        if activation is None:\n",
    "            self.weight_init()\n",
    "        elif activation=='sigmoid':\n",
    "            self.weight_init(method='Xavier')\n",
    "        elif activation=='relu':\n",
    "            self.weight_init(method='He')\n",
    "\n",
    "    # def weight_init(self, weight_init_std=0.01, seed=42):\n",
    "    #     np.random.seed(seed)\n",
    "    #     self.params = {'weights': OrderedDict(), 'biases': OrderedDict()}\n",
    "\n",
    "    #     for i in range(self.n_layers):\n",
    "    #         self.params['weights'][f'w{i}'] = weight_init_std * np.random.randn(self.node_sizes[i], self.node_sizes[i+1])\n",
    "    #         self.params['biases'] [f'b{i}'] = np.zeros(self.node_sizes[i+1])\n",
    "\n",
    "    # Xavier, He, Default(torch에서 사용하는 방식)\n",
    "    # > 책 205p\n",
    "    # > torch 공식문서\n",
    "    #   - nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    #   - nn.Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    def weight_init(self, method=None, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.params = {'weights': OrderedDict(), 'biases': OrderedDict()}\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            input_dim = self.node_sizes[i]\n",
    "            output_dim = self.node_sizes[i+1]\n",
    "\n",
    "            if method is None:\n",
    "                limit = 0.01\n",
    "            elif method == 'Xavier':\n",
    "                limit = np.sqrt(1 / input_dim)   # Xavier initialization\n",
    "            elif method == 'He':\n",
    "                limit = np.sqrt(2 / input_dim)   # He initialization\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            self.params['weights'][f'w{i}'] = np.random.randn(input_dim, output_dim) * limit\n",
    "            self.params['biases'] [f'b{i}'] = np.random.randn(output_dim) * limit\n",
    "\n",
    "    def predict(self, x, params=None, return_activations=False):\n",
    "        if params is None:\n",
    "            weights = deepcopy(self.params['weights'])\n",
    "            biases = deepcopy(self.params['biases'])\n",
    "        else:\n",
    "            weights = params['weights']\n",
    "            biases = params['biases']\n",
    "\n",
    "        if return_activations:\n",
    "            activations = []\n",
    "\n",
    "        # 초기 activation은 x로 초기화\n",
    "        activation = x\n",
    "        \n",
    "        # layer들의 선형결합\n",
    "        for j, ((w_name, w_ij), (b_name, b_j)) in enumerate(zip(weights.items(), biases.items())):\n",
    "            # logit (linear combination)\n",
    "            logits = activation @ w_ij + b_j\n",
    "\n",
    "            # activation function\n",
    "            if j+1 != self.n_layers:\n",
    "                activation = self.activation(logits)\n",
    "            else:\n",
    "                activation = logits\n",
    "            \n",
    "            if return_activations:\n",
    "                activations.append(activation)\n",
    "\n",
    "        if return_activations:\n",
    "            return activation, activations\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "    def loss(self, x, y_true, params=None):\n",
    "        activation = self.predict(x, params)\n",
    "        prob = softmax(activation)\n",
    "        loss = cross_entropy_error(y_true=y_true, y_pred=prob)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, x, y_true):\n",
    "        y_pred = self.predict(x)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        y_true = np.argmax(y_true, axis=1)\n",
    "        \n",
    "        acc = np.sum(y_pred==y_true) / len(y_pred)\n",
    "        return acc\n",
    "\n",
    "    def _compute_numerical_gradient(self, x, y_true, name, idx, what):\n",
    "        h = 1e-4\n",
    "\n",
    "        # set params\n",
    "        params = {'weights': deepcopy(self.params['weights']), 'biases': deepcopy(self.params['biases'])}\n",
    "        val = params[what][name][idx]\n",
    "\n",
    "        # f(x+h)\n",
    "        params[what][name][idx] = val + h\n",
    "        fxh1 = self.loss(x, y_true, params)\n",
    "        # f(x-h)\n",
    "        params[what][name][idx] = val - h\n",
    "        fxh2 = self.loss(x, y_true, params)\n",
    "\n",
    "        # ( f(x+h) - f(x-h) ) / 2h\n",
    "        return (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "    def numerical_gradient(self, x, y_true, max_workers=1):\n",
    "        Executor = ThreadPoolExecutor # ThreadPoolExecutor. ProcessPoolExecutor\n",
    "        params = {'weights': OrderedDict(), 'biases': OrderedDict()}\n",
    "        weights = self.params['weights']\n",
    "        biases = self.params['biases']\n",
    "\n",
    "        for j, ((w_name, w_ij), (b_name, b_j)) in enumerate(zip(weights.items(), biases.items())):\n",
    "            \n",
    "            # (1) weight gradients\n",
    "            iterable = np.ndindex(weights[w_name].shape)\n",
    "            total = weights[w_name].size\n",
    "            desc = f'[{j+1}/{len(weights)}] - weights'\n",
    "\n",
    "            grad_weight = np.zeros_like(weights[w_name], dtype=np.float64)\n",
    "\n",
    "            if max_workers==1:\n",
    "                pbar = tqdm(iterable, total=total, desc=desc)\n",
    "                for idx in pbar:\n",
    "                    grad_weight[idx] = self._compute_numerical_gradient(x, y_true, w_name, idx, 'weights')\n",
    "            else:\n",
    "                with Executor(max_workers=max_workers) as executor:\n",
    "                    futures = {\n",
    "                        executor.submit(self._compute_numerical_gradient, x, y_true, w_name, idx, 'weights'): idx\n",
    "                        for idx in iterable\n",
    "                    }\n",
    "                    #pbar = tqdm(as_completed(futures), total=total, desc=desc, position=0, leave=False)\n",
    "                    pbar = as_completed(futures)\n",
    "                    for future in pbar:\n",
    "                        idx = futures[future]\n",
    "                        grad_weight[idx] = future.result()\n",
    "\n",
    "            params['weights'][w_name] = grad_weight\n",
    "\n",
    "            # (2) bias gradients\n",
    "            grad_bias = np.zeros_like(biases[b_name], dtype=np.float64)\n",
    "\n",
    "            iterable = np.ndindex(biases[b_name].shape)\n",
    "            total = biases[b_name].size\n",
    "            desc = f'[{j+1}/{len(biases)}] - biases'\n",
    "\n",
    "            if max_workers==1:\n",
    "                pbar = tqdm(iterable, total=total, desc=desc)\n",
    "                for idx in pbar:\n",
    "                    grad_bias[idx] = self._compute_numerical_gradient(x, y_true, b_name, idx, 'biases')\n",
    "            else:\n",
    "                with Executor(max_workers=max_workers) as executor:\n",
    "                    futures = {\n",
    "                        executor.submit(self._compute_numerical_gradient, x, y_true, b_name, idx, 'biases'): idx\n",
    "                        for idx in pbar\n",
    "                    }\n",
    "                    #pbar = tqdm(as_completed(futures), total=total, desc=desc, position=0, leave=False)\n",
    "                    pbar = as_completed(futures)\n",
    "                    for future in pbar:\n",
    "                        idx = futures[future]\n",
    "                        grad_bias[idx] = future.result()\n",
    "\n",
    "            params['biases'][b_name] = grad_bias\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "input_size = 50\n",
    "output_size = 3\n",
    "hidden_sizes = [16]\n",
    "\n",
    "net = MultiLayerNet(input_size, output_size, hidden_sizes, activation=None)\n",
    "print(f\"> node sizes: {net.node_sizes}\")\n",
    "print(f\"> weights sizes: {[v.shape for k,v in net.params['weights'].items()]}\")\n",
    "print(f\"> bias sizes: {[v.shape for k,v in net.params['biases'].items()]}\")\n",
    "\n",
    "x = np.random.rand(n, input_size)\n",
    "y_true = np.random.rand(n, output_size)\n",
    "\n",
    "y_pred = net.predict(x)\n",
    "print(f\"> y_pred size: {y_pred.shape}\")\n",
    "\n",
    "loss = net.loss(x, y_true)\n",
    "print(f\"> loss: {loss:.4f}\")\n",
    "\n",
    "acc = net.accuracy(x, y_true)\n",
    "print(f\"> accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu인 경우 출력값이 음수인 경우 0이 되므로 가중치가 0이 될 수 있음\n",
    "numerical_grads = net.numerical_gradient(x, y_true, max_workers=8)\n",
    "numerical_grads['weights']['w0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미니배치 구현 : 141p\n",
    "\n",
    "def label_onehot(label, num_classes):\n",
    "    label = np.array(label)\n",
    "    label = label - min(label)\n",
    "    onehot = np.zeros((len(label), num_classes))\n",
    "    for i, l in enumerate(label):\n",
    "        onehot[i, l] = 1\n",
    "    return onehot\n",
    "\n",
    "# # HTTP 403 에러 뜸\n",
    "# from dataset.mnist import load_mnist\n",
    "# (X_train, y_train), (X_test, y_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.astype(np.float64)\n",
    "X_test = X_test.astype(np.float64)\n",
    "\n",
    "# normalize\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# X -> flatten\n",
    "X_train = X_train.reshape(X_train.shape[0], -1)\n",
    "X_test  = X_test .reshape(X_test .shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_workers = os.cpu_count() // 2\n",
    "# iters_num = 100\n",
    "# train_size = X_train.shape[0]\n",
    "# batch_size = 100\n",
    "# lr = 0.1\n",
    "\n",
    "# input_size = 784\n",
    "# output_size = 10\n",
    "# hidden_sizes = [50]\n",
    "\n",
    "# model = MultiLayerNet(input_size, output_size, hidden_sizes, activation=None)\n",
    "\n",
    "# elapsed_times = []\n",
    "# train_loss_list = []\n",
    "# val_loss_list = []\n",
    "# for i in range(iters_num):\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     # get mini-batch (실제로는 모든 6만개의 데이터에 대해서 모두 가중치 업데이트를 해야 1개의 epoch로 쳐줌)\n",
    "#     batch_mask = np.random.choice(train_size, batch_size)\n",
    "#     X_batch, y_batch = X_train[batch_mask], y_train[batch_mask]\n",
    "    \n",
    "#     # calculate gradients\n",
    "#     grads = model.numerical_gradient(X_batch, y_batch, max_workers=max_workers)\n",
    "\n",
    "#     # gradient descent\n",
    "#     for grad_key in grads.keys():\n",
    "#         for key in grads[grad_key]:\n",
    "#             model.params[grad_key][key] -= lr * grads[grad_key][key]\n",
    "\n",
    "#     # loss\n",
    "#     train_loss = model.loss(X_batch, y_batch)\n",
    "#     val_loss   = model.loss(X_test, y_test)\n",
    "\n",
    "#     train_loss_list.append(train_loss)\n",
    "#     val_loss_list  .append(val_loss)\n",
    "\n",
    "#     # progress\n",
    "#     now_time = str(datetime.datetime.now())[:-7]\n",
    "#     end_time = time.time()\n",
    "#     elapsed = end_time-start_time\n",
    "#     elapsed_times.append(elapsed)\n",
    "#     total = sum(elapsed_times)\n",
    "#     remaining = (iters_num-i-1) * elapsed\n",
    "\n",
    "#     str_i = str(i+1).zfill(len(str(iters_num)))\n",
    "#     progress = f'{now_time} | [{str_i}/{iters_num}] {train_loss=:.4f}, {val_loss=:.4f}, {elapsed=:.1f}s, {total=:.1f}s, {remaining=:.1f}s'\n",
    "#     print(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 : 161p\n",
    "class MulLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x * y # 순전파는 곱셈으로 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * self.y # x와 y를 바꾸어서 미분값에 곱함\n",
    "        dy = dout * self.x\n",
    "        return dx, dy\n",
    "\n",
    "# 덧셈 계층 : 163p\n",
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        return x + y # 순전파는 덧셈으로 반환\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = dout * 1\n",
    "        dy = dout * 1\n",
    "        return dx, dy # 값 그대로(1을 곱한채로) 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈 계층 예제 : 162p\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "# (1) forward : input 2, output 1\n",
    "apple = 100\n",
    "apple_num = 2\n",
    "tax = 1.1\n",
    "\n",
    "apple_price = mul_apple_layer.forward(apple, apple_num) # (1-1) 사과가격(100) * 사과개수(2)\n",
    "price = mul_tax_layer.forward(apple_price, tax)         # (1-2) 전체사과가격(200) * 세금(1.1)\n",
    "print(price)\n",
    "\n",
    "# (2) backward : input 1, output 2\n",
    "dprice = 1\n",
    "\n",
    "dapple_price, dtax = mul_tax_layer.backward(dprice)         # (2-2) 전체가격 -> 세금\n",
    "dapple, dapple_num = mul_apple_layer.backward(dapple_price) # (2-1) 세금 -> 사과가격,개수\n",
    "print(dapple, dapple_num, dtax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 곱셈+덧셈 계층 예제 : 164p\n",
    "\n",
    "# (1) forward\n",
    "\n",
    "# (1-1) 사과와 오렌지의 각각의 가격 및 개수\n",
    "apple_price, apple_num = 100, 2   # 100원짜리 사과 2개\n",
    "orange_price, orange_num = 150, 3 # 150원짜리 오렌지 3개\n",
    "\n",
    "mul_apple_layer = MulLayer()\n",
    "mul_orange_layer = MulLayer()\n",
    "\n",
    "tot_apple_price = mul_apple_layer.forward(apple_price, apple_num)\n",
    "tot_orange_price = mul_orange_layer.forward(orange_price, orange_num)\n",
    "\n",
    "# (1-2) 사과와 오렌지의 전체 가격 합치기\n",
    "add_apple_orange_layer = AddLayer()\n",
    "\n",
    "tot_price = add_apple_orange_layer.forward(tot_apple_price, tot_orange_price)\n",
    "\n",
    "# (1-3) 소비세 곱하기\n",
    "tax_ratio = 1.1\n",
    "\n",
    "mul_tax_layer = MulLayer()\n",
    "\n",
    "final_price = mul_tax_layer.forward(tot_price, tax_ratio)\n",
    "\n",
    "# forward result\n",
    "print(f'\\n> forward',\n",
    "      f'\\n   step1: {tot_apple_price=:.2f}, {tot_orange_price=:.2f},',\n",
    "      f'\\n   step2: {tot_price=:.2f},',\n",
    "      f'\\n   step3: {final_price=:.2f}')\n",
    "\n",
    "# (2) backward\n",
    "# - forward와는 순서가 반대로\n",
    "# - forward에 input이었던 것들이 output으로, output이었던 것이 input으로\n",
    "\n",
    "# (2-1) 전체 가격과 세금 나누기\n",
    "dfinal_price = 1\n",
    "\n",
    "dtot_price, dtax_ratio = mul_tax_layer.backward(dfinal_price)\n",
    "\n",
    "# (2-2) 사과와 오렌지의 전체 가격 나누기\n",
    "dtot_apple_price, dtot_orange_price = add_apple_orange_layer.backward(dtot_price)\n",
    "\n",
    "# (2-3) 사과와 오렌지의 가격 및 개수 나누기\n",
    "dapple_price, dapple_num = mul_apple_layer.backward(dtot_apple_price)\n",
    "dorange_price, dorange_num = mul_orange_layer.backward(dtot_orange_price)\n",
    "\n",
    "# backward result\n",
    "print(f'\\n> backward',\n",
    "      f'\\n   step1: {dtot_price=:.2f}, {dtax_ratio=:.2f},',\n",
    "      f'\\n   step2: {dtot_apple_price=:.2f}, {dtot_orange_price=:.2f},',\n",
    "      f'\\n   step3: {dapple_price=:.2f}, {dapple_num=:.2f}, {dorange_price=:.2f}, {dorange_num=:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu 계층 구현 : 166p\n",
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.negative_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.array(x)\n",
    "        self.negative_mask = (x <= 0)\n",
    "        out = np.where(self.negative_mask, 0, x) # 0보다 큰 값은 값그대로, 0보다 작은 값은 0으로\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = np.array(dout)\n",
    "        dx = np.where(self.negative_mask, 0, dout) # 0보다 큰 값은 값그대로, 0보다 작은 값은 0으로 (dout 값에 relu_derivative를 곱해줌)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,num=100)\n",
    "activation = ReLU()\n",
    "dout = activation.forward(x)\n",
    "dx = activation.backward(dout)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(x, dout)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dout')\n",
    "plt.title('x vs dout')\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(x, dx)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dx')\n",
    "plt.title('x vs dx')\n",
    "plt.suptitle(f'[{activation.__class__.__name__}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU:\n",
    "    def __init__(self, negative_slope=0.01):\n",
    "        self.negative_slope = negative_slope\n",
    "        self.negative_mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.array(x)\n",
    "        self.negative_mask = (x <= 0)\n",
    "        out = np.where(self.negative_mask, self.negative_slope*x, x) # 0보다 큰 값은 값그대로, 0보다 작은 값은 negative slope를 곱한 값으로\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = np.array(dout)\n",
    "        dx = np.where(self.negative_mask, self.negative_slope*dout, dout) # 0보다 큰 값은 값그대로, 0보다 작은 값은 negative slope를 곱한 값으로 (dout 값에 leakyrelu_derivative를 곱해줌)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,num=100)\n",
    "activation = LeakyReLU(negative_slope=0.1)\n",
    "dout = activation.forward(x)\n",
    "dx = activation.backward(dout)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(x, dout)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dout')\n",
    "plt.title('x vs dout')\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(x, dx)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dx')\n",
    "plt.title('x vs dx')\n",
    "plt.suptitle(f'[{activation.__class__.__name__}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid 계층 구현 : 170p\n",
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.array(x)\n",
    "        self.out = np.where(x>0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(-x))) # overflow 방지\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = np.array(dout)\n",
    "        dx = dout * self.out * (1.0-self.out)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,num=100)\n",
    "activation = Sigmoid()\n",
    "dout = activation.forward(x)\n",
    "dx = activation.backward(dout)\n",
    "\n",
    "fig = plt.figure(figsize=(10,4))\n",
    "fig.add_subplot(1,2,1)\n",
    "plt.plot(x, dout)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dout')\n",
    "plt.title('x vs dout')\n",
    "fig.add_subplot(1,2,2)\n",
    "plt.plot(x, dx)\n",
    "plt.grid(alpha=0.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('dx')\n",
    "plt.title('x vs dx')\n",
    "plt.suptitle(f'[{activation.__class__.__name__}]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affine 계층 구현 : 175p\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = np.array(W)\n",
    "        self.b = np.array(b)\n",
    "\n",
    "        # 가중치와 편향의 gradient descent를 위해 각각의 기울기를 저장\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.X = np.array(X)\n",
    "        self.origin_shape = self.X.shape\n",
    "\n",
    "        # 다차원인 경우, batch에 대해서 적용하기위해 평탄화(2D로 변환) 시켜줌\n",
    "        if self.X.ndim > 2:\n",
    "            self.X = self.X.reshape(self.X.shape[0], -1)\n",
    "        \n",
    "        out = self.X @ self.W + self.b # out = XW + b\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = np.array(dout)\n",
    "        self.dW = self.X.T @ dout        # dL/dW = X^T * dL/dY\n",
    "        self.db = np.sum(dout, axis=0)   # dL/dB = dL/dY = sum(dL_j/dY)\n",
    "        dX = dout @ self.W.T             # dL/dX = dL/dY * W^T\n",
    "        dX = dX.reshape(self.origin_shape)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "input_dim = 3\n",
    "output_dim = 4\n",
    "\n",
    "W = np.random.randn(input_dim,output_dim) # W : (input_size, output_size)\n",
    "b = np.zeros(output_dim)                  # b : (output_size)\n",
    "X = np.random.randn(batch_size,input_dim) # X : (batch_size, input_size)\n",
    "\n",
    "affine = Affine(W,b)\n",
    "print(affine.forward(X).shape)            # forward : (batch_size, output_size) = (batch_size, input_size) @ (input_size, output_sizse)\n",
    "dout = np.ones((batch_size,output_dim))\n",
    "print(affine.backward(dout).shape)        # backward : (batch_size, input_size)\n",
    "print(affine.dW.shape, affine.db.shape)   # shape of dW, db = shape of W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 참조 : https://slamwithme.oopy.io/305bb7e0-1062-4785-a82e-9e2a5debd0f4\n",
    "\n",
    "# (1) Binary Class\n",
    "#   (1-1) torch.nn.BCELoss() : Layer(affine, activation, batchnorm 등의 조합) 마지막에 sigmoid 추가 필요\n",
    "#   (1-2) torch.nn.BCEWithLogitsLoss() : Layer 마지막에 sigmoid 추가 불필요 (sigmoid가 포함되어있음)\n",
    "#\n",
    "\n",
    "# (2) Multiple Class\n",
    "#   : torch.nn.CrossEntropyLoss() : Layer 마지막에 softmax 추가 불필요 (softmax가 포함되어있음)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax with loss 계층 구현 : 179p\n",
    "# -> torch.nn.CrossEntropyLoss() 구현 예제\n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.y = None\n",
    "        self.t = None\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        self.y = softmax(np.array(y))\n",
    "        self.t = np.array(t)\n",
    "\n",
    "        # cross entropy loss\n",
    "        self.loss = cross_entropy_error(y_pred=self.y, y_true=self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self): # dout의 영향을 받지 않음\n",
    "        # true값이 onehot인 경우\n",
    "        batch_size = self.t.shape[0]\n",
    "        if self.y.size == self.t.size:\n",
    "            dx = (self.y - self.t)\n",
    "        # true값이 정수형인 경우\n",
    "        else:\n",
    "            dx = self.y.copy()\n",
    "            dx[np.arange(batch_size), self.t] -= 1  # 예측 확률에서 실제 정수형 레이블에 해당하는 값을 1 뺌\n",
    "\n",
    "        # 오차를 batch size로 나눠서 역전파\n",
    "        dx /= batch_size\n",
    "\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역전파를 이용한 MultiLayerNet 구현 : 182p\n",
    "from tqdm import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "class MultiLayerNet:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation=None, weight_init='auto', weight_init_limit=None):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.weight_init_limit = weight_init_limit\n",
    "\n",
    "        self.node_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.n_layers = len(self.node_sizes)-1\n",
    "\n",
    "        # activation\n",
    "        if activation is None:\n",
    "            activation = 'sigmoid'\n",
    "        else:\n",
    "            activation = activation.lower()\n",
    "\n",
    "        activation_dict = {\n",
    "            'sigmoid' : Sigmoid(),\n",
    "            'relu' : ReLU(),\n",
    "        }\n",
    "\n",
    "        self.activation = activation_dict.get(activation)\n",
    "\n",
    "        # weight init\n",
    "        if weight_init=='auto':\n",
    "            if activation=='sigmoid':\n",
    "                self.weight_init(method='Xavier')\n",
    "            elif activation=='relu':\n",
    "                self.weight_init(method='He')\n",
    "        else:\n",
    "            self.weight_init(method=weight_init)\n",
    "\n",
    "        # define layers\n",
    "        self.layers_backward = OrderedDict()\n",
    "        for i in range(self.n_layers):\n",
    "            W = self.params['weights'][f'w{i+1}']\n",
    "            b = self.params['biases'] [f'b{i+1}']\n",
    "            self.layers_backward[f'Affine{i+1}'] = Affine(W,b)\n",
    "            if i+1 != self.n_layers:\n",
    "                self.layers_backward[f'Activation{i+1}'] = deepcopy(self.activation)\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    # > 책 205p\n",
    "    # > torch 공식문서\n",
    "    #   - nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    #   - nn.Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    def weight_init(self, method=None, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.params = {'weights': OrderedDict(), 'biases': OrderedDict()}\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            input_dim = self.node_sizes[i]\n",
    "            output_dim = self.node_sizes[i+1]\n",
    "\n",
    "            if method is None:\n",
    "                limit = self.weight_init_limit\n",
    "            elif method == 'Xavier':\n",
    "                limit = np.sqrt(1 / input_dim)   # Xavier initialization\n",
    "            elif method == 'He':\n",
    "                limit = np.sqrt(2 / input_dim)   # He initialization\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            self.params['weights'][f'w{i+1}'] = np.random.randn(input_dim, output_dim) * limit\n",
    "            self.params['biases'] [f'b{i+1}'] = np.random.randn(output_dim) * limit\n",
    "\n",
    "    def predict_numerical_gradient(self, x, params=None, return_activations=False):\n",
    "        if params is None:\n",
    "            weights = deepcopy(self.params['weights'])\n",
    "            biases = deepcopy(self.params['biases'])\n",
    "        else:\n",
    "            weights = params['weights']\n",
    "            biases = params['biases']\n",
    "\n",
    "        if return_activations:\n",
    "            activations = []\n",
    "\n",
    "        # 초기 activation은 x로 초기화\n",
    "        activation = x\n",
    "        \n",
    "        # layer들의 선형결합\n",
    "        for j, ((w_name, w_ij), (b_name, b_j)) in enumerate(zip(weights.items(), biases.items())):\n",
    "            # logit (linear combination)\n",
    "            logits = activation @ w_ij + b_j\n",
    "\n",
    "            # activation function\n",
    "            if j+1 != self.n_layers:\n",
    "                activation = self.activation.forward(logits)\n",
    "            else:\n",
    "                activation = logits\n",
    "            \n",
    "            if return_activations:\n",
    "                activations.append(activation)\n",
    "\n",
    "        if return_activations:\n",
    "            return activation, activations\n",
    "        else:\n",
    "            return activation\n",
    "\n",
    "    def loss_numerical_gradient(self, x, y_true, params=None):\n",
    "        activation = self.predict_numerical_gradient(x, params)\n",
    "        prob = softmax(activation)\n",
    "        loss = cross_entropy_error(y_true=y_true, y_pred=prob)\n",
    "        return loss\n",
    "\n",
    "    def _compute_numerical_gradient(self, x, y_true, name, idx, what):\n",
    "        h = 1e-4\n",
    "\n",
    "        # set params\n",
    "        params = {'weights': deepcopy(self.params['weights']), 'biases': deepcopy(self.params['biases'])}\n",
    "        val = params[what][name][idx]\n",
    "\n",
    "        # f(x+h)\n",
    "        params[what][name][idx] = val + h\n",
    "        fxh1 = self.loss_numerical_gradient(x, y_true, params)\n",
    "        # f(x-h)\n",
    "        params[what][name][idx] = val - h\n",
    "        fxh2 = self.loss_numerical_gradient(x, y_true, params)\n",
    "\n",
    "        # ( f(x+h) - f(x-h) ) / 2h\n",
    "        return (fxh1 - fxh2) / (2*h)\n",
    "\n",
    "    def numerical_gradient(self, x, y_true, max_workers=1):\n",
    "        Executor = ThreadPoolExecutor # ThreadPoolExecutor. ProcessPoolExecutor\n",
    "        params = {'weights': OrderedDict(), 'biases': OrderedDict()}\n",
    "        weights = self.params['weights']\n",
    "        biases = self.params['biases']\n",
    "\n",
    "        for j, ((w_name, w_ij), (b_name, b_j)) in enumerate(zip(weights.items(), biases.items())):\n",
    "            \n",
    "            # iterable define\n",
    "            iterable_weight = np.ndindex(weights[w_name].shape)\n",
    "            iterable_bias = np.ndindex(biases[b_name].shape)\n",
    "\n",
    "            # pbar args\n",
    "            total_weight = weights[w_name].size\n",
    "            total_bias   = biases[b_name].size\n",
    "            desc_weight = f'[{j+1}/{len(weights)}] - weights'\n",
    "            desc_bias   = f'[{j+1}/{len(biases)}] - biases'\n",
    "\n",
    "            # grad init\n",
    "            grad_weight = np.zeros_like(weights[w_name], dtype=np.float64)\n",
    "            grad_bias = np.zeros_like(biases[b_name], dtype=np.float64)\n",
    "\n",
    "            if max_workers==1:\n",
    "                pbar = tqdm(iterable_weight, total=total_weight, desc=desc_weight)\n",
    "                for idx in pbar:\n",
    "                    grad_weight[idx] = self._compute_numerical_gradient(x, y_true, w_name, idx, 'weights')\n",
    "\n",
    "                pbar = tqdm(iterable_bias, total=total_bias, desc=desc_bias)\n",
    "                for idx in pbar:\n",
    "                    grad_bias[idx] = self._compute_numerical_gradient(x, y_true, b_name, idx, 'biases')\n",
    "\n",
    "            else:\n",
    "                with Executor(max_workers=max_workers) as executor:\n",
    "                    futures_weight = {\n",
    "                        executor.submit(self._compute_numerical_gradient, x, y_true, w_name, idx, 'weights'): idx\n",
    "                        for idx in iterable_weight\n",
    "                    }\n",
    "                    futures_bias = {\n",
    "                        executor.submit(self._compute_numerical_gradient, x, y_true, b_name, idx, 'biases'): idx\n",
    "                        for idx in iterable_bias\n",
    "                    }\n",
    "\n",
    "                    pbar = tqdm(as_completed(futures_weight), total=total_weight, desc=desc_weight, position=0, leave=False)\n",
    "                    # pbar = as_completed(futures_weight)\n",
    "                    for future_weight in pbar:\n",
    "                        idx = futures_weight[future_weight]\n",
    "                        grad_weight[idx] = future_weight.result()\n",
    "\n",
    "                    pbar = tqdm(as_completed(futures_bias), total=total_bias, desc=desc_bias, position=0, leave=False)\n",
    "                    # pbar = as_completed(futures_bias)\n",
    "                    for future_bias in pbar:\n",
    "                        idx = futures_bias[future_bias]\n",
    "                        grad_bias[idx] = future_bias.result()\n",
    "\n",
    "            # get gradient\n",
    "            params['weights'][w_name] = grad_weight\n",
    "            params['biases'][b_name] = grad_bias\n",
    "\n",
    "        return params\n",
    "\n",
    "    def predict_backward(self, x):\n",
    "        for layer in self.layers_backward.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss_backward(self, x, y_true):\n",
    "        y_pred = self.predict_backward(x)\n",
    "        loss = self.last_layer.forward(y=y_pred, t=y_true)\n",
    "        return loss\n",
    "\n",
    "    def gradient(self, x, y_true):\n",
    "        params = {'weights': {}, 'biases': {}}\n",
    "\n",
    "        # 순전파\n",
    "        self.loss_backward(x, y_true)\n",
    "\n",
    "        # 역전파\n",
    "        dout = self.last_layer.backward()\n",
    "\n",
    "        for key, layer in reversed(self.layers_backward.items()):\n",
    "            dout = layer.backward(dout)\n",
    "            if key.startswith('Affine'):\n",
    "                i = int(key.replace('Affine',''))\n",
    "                params['weights'][f'w{i}'] = layer.dW\n",
    "                params['biases'] [f'b{i}'] = layer.db\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_split(X, y, batch_size, shuffle=False, random_state=42):\n",
    "    if shuffle:\n",
    "        np.random.seed(random_state)\n",
    "        shuffled_indices = np.random.permutation(len(X))\n",
    "        X = X[shuffled_indices]\n",
    "        y = y[shuffled_indices]\n",
    "\n",
    "    return [\n",
    "        [X[i:i + batch_size] for i in range(0, len(X), batch_size)],\n",
    "        [y[i:i + batch_size] for i in range(0, len(X), batch_size)],\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer : 191p ~ 199p\n",
    "\n",
    "# https://github.com/WegraLee/deep-learning-from-scratch/blob/master/common/optimizer.py\n",
    "# *NAG(모멘텀에서 한단계 발전한 방법)은 위 사이트 참조\n",
    "__all__ = ['SGD', 'Momentum', 'AdaGrad', 'RMSProp', 'Adam']\n",
    "print(f'{__all__ = }')\n",
    "\n",
    "params_dict = {'weights':{}, 'biases':{}, 'gamma':{}, 'beta':{}}\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        for key1 in grads.keys():\n",
    "            for key2 in grads[key1].keys():\n",
    "                params[key1][key2] -= lr * grads[key1][key2]\n",
    "        return params\n",
    "\n",
    "# 기울기 방향으로 가속\n",
    "class Momentum:\n",
    "    def __init__(self, lr=0.01, momentum=0.9):\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.v = None\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.v is None:\n",
    "            self.v = params_dict\n",
    "            for key1 in grads.keys():\n",
    "                for key2 in grads[key1].keys():\n",
    "                    self.v[key1][key2] = np.zeros_like(grads[key1][key2])\n",
    "\n",
    "        for key1 in grads.keys():\n",
    "            for key2 in grads[key1].keys():\n",
    "                # 초기에는 v의 모든값이 0으로 SGD와 같지만, 업데이트됨에따라 momentum이 더해져 가속됨\n",
    "                self.v[key1][key2] = (self.momentum * self.v[key1][key2]) - (lr * grads[key1][key2]) \n",
    "                params[key1][key2] += self.v[key1][key2]\n",
    "\n",
    "        return params\n",
    "\n",
    "# learning rate가 너무 작으면 학습이 느리고, 너무 크면 발산함\n",
    "# -> 학습을 진행하면서 학습률을 점차 줄여감\n",
    "class AdaGrad:\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.h = None\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = params_dict\n",
    "            for key1 in grads.keys():\n",
    "                for key2 in grads[key1].keys():\n",
    "                    self.h[key1][key2] = np.zeros_like(grads[key1][key2])\n",
    "        \n",
    "        for key1 in grads.keys():\n",
    "            for key2 in grads[key1].keys():\n",
    "                self.h[key1][key2] += grads[key1][key2]**2 # 가중치 감쇠\n",
    "                params[key1][key2] -= lr * grads[key1][key2] / (np.sqrt(self.h[key1][key2]) + self.eps)\n",
    "                \n",
    "        return params\n",
    "\n",
    "# AdaGrad는 기울기를 제곱해서 계속 더해가므로, 무한히 학습한다면 갱신량이 0으로 수렴함\n",
    "# -> 이 문제를 개선한 방법으로 RMSProp이 제안됨\n",
    "class RMSProp:\n",
    "    def __init__(self, lr=0.01, decay_rate=0.99):\n",
    "        self.lr = lr\n",
    "        self.decay_rate = decay_rate\n",
    "        self.h = None\n",
    "        self.eps = 1e-7\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        if self.h is None:\n",
    "            self.h = params_dict\n",
    "            for key1 in grads.keys():\n",
    "                for key2 in grads[key1].keys():\n",
    "                    self.h[key1][key2] = np.zeros_like(grads[key1][key2])\n",
    "        \n",
    "        for key1 in grads.keys():\n",
    "            for key2 in grads[key1].keys():\n",
    "                self.h[key1][key2] += (self.decay_rate) * (1-self.decay_rate) * (grads[key1][key2]**2) # 가중치 감쇠\n",
    "                params[key1][key2] -= lr * grads[key1][key2] / (np.sqrt(self.h[key1][key2]) + self.eps)\n",
    "                \n",
    "        return params\n",
    "\n",
    "# Momentum + RMSProp\n",
    "class Adam:\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.iter = 0\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.eps = 1e-2 #1e-7\n",
    "\n",
    "    def update(self, params, grads):\n",
    "        self.iter += 1\n",
    "        lr_iter = self.lr * (1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)\n",
    "\n",
    "        if self.m is None:\n",
    "            self.m = params_dict\n",
    "            self.v = params_dict\n",
    "            for key1 in grads.keys():\n",
    "                for key2 in grads[key1].keys():\n",
    "                    self.m[key1][key2] = np.zeros_like(grads[key1][key2])\n",
    "                    self.v[key1][key2] = np.zeros_like(grads[key1][key2])\n",
    "\n",
    "        for key1 in grads.keys():\n",
    "            for key2 in grads[key1].keys():\n",
    "                #(기존 Adam 알고리즘)\n",
    "                # # 모멘텀 업데이트\n",
    "                # self.m[key1][key2] = (self.beta1 * self.m[key1][key2]) + ((1.0 - self.beta1) * (grads[key1][key2]))\n",
    "                # self.v[key1][key2] = (self.beta2 * self.v[key1][key2]) + ((1.0 - self.beta2) * (grads[key1][key2]**2))\n",
    "\n",
    "                # # 편향 보정\n",
    "                # m_hat = self.m[key1][key2] / (1.0 - (self.beta1**self.iter))\n",
    "                # v_hat = self.v[key1][key2] / (1.0 - (self.beta2**self.iter))\n",
    "\n",
    "                #(차분방식 Adam 알고리즘)\n",
    "                self.m[key1][key2] += (1.0 - self.beta1) * (grads[key1][key2] - self.m[key1][key2])\n",
    "                self.v[key1][key2] += (1.0 - self.beta1) * (grads[key1][key2]**2 - self.v[key1][key2])\n",
    "\n",
    "                # 가중치 업데이트\n",
    "                print(self.v[key1][key2][:5])\n",
    "                params[key1][key2] -= lr_iter * self.m[key1][key2] / (np.sqrt(self.v[key1][key2]) + self.eps)\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_sizes = [8]\n",
    "\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid')\n",
    "X_batches, y_batches = batch_split(X_train, y_train, batch_size, shuffle=True, random_state=42)\n",
    "\n",
    "i=0\n",
    "X_batch = X_batches[i]\n",
    "y_batch = y_batches[i]\n",
    "\n",
    "# predict 확인\n",
    "a = model.predict_backward(X_batch)\n",
    "b = model.predict_numerical_gradient(X_batch)\n",
    "print('predict diff:',np.sum((a-b)**2))\n",
    "\n",
    "# loss 확인\n",
    "a = model.loss_backward(X_batch, y_batch)\n",
    "b = model.loss_numerical_gradient(X_batch, y_batch)\n",
    "print('loss diff:',a-b)\n",
    "\n",
    "# gradient 확인\n",
    "grads1 = model.gradient(X_batch, y_batch)\n",
    "grads2 = model.numerical_gradient(X_batch, y_batch, max_workers=1)\n",
    "for key1 in grads2.keys():\n",
    "    for key2 in grads2[key1].keys():\n",
    "        diff = np.mean((grads1[key1][key2] - grads2[key1][key2])**2)\n",
    "        print(key1,key2,diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Custom Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 100\n",
    "# batch_size = 64\n",
    "# lr = 0.1\n",
    "\n",
    "# input_size = 784\n",
    "# output_size = 10\n",
    "# hidden_sizes = [8]\n",
    "\n",
    "# model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid')\n",
    "# optimizer = Adam(lr=lr) # SGD, Momentum, AdaGrad, RMSProp, Adam\n",
    "\n",
    "# X_batches, y_batches = batch_split(X_train, y_train, batch_size, shuffle=True, random_state=42)\n",
    "\n",
    "# elapsed_times = []\n",
    "# train_loss_list = []\n",
    "# val_loss_list = []\n",
    "# for i in range(epochs):\n",
    "#     start_time = time.time()\n",
    "\n",
    "#     train_loss = 0\n",
    "#     val_loss = 0\n",
    "\n",
    "#     # get mini-batch (실제로는 모든 6만개의 데이터에 대해서 모두 가중치 업데이트를 해야 1개의 epoch로 쳐줌)\n",
    "#     pbar_batch = tqdm(zip(X_batches, y_batches), total=len(X_batches))\n",
    "#     for X_batch, y_batch in pbar_batch:\n",
    "#         # calculate gradients\n",
    "#         grads = model.gradient(X_batch, y_batch)\n",
    "\n",
    "#         # gradient descent\n",
    "#         model.params = optimizer.update(model.params, grads)\n",
    "\n",
    "#         for k in range(model.n_layers):\n",
    "#             W = model.params['weights'][f'w{k+1}']\n",
    "#             b = model.params['biases'] [f'b{k+1}']\n",
    "#             model.layers_backward[f'Affine{k+1}'] = Affine(W, b)\n",
    "\n",
    "#         # loss\n",
    "#         train_loss += model.loss_backward(X_batch, y_batch)\n",
    "#         val_loss   += model.loss_backward(X_test, y_test)\n",
    "\n",
    "#     train_loss /= len(X_batches)\n",
    "#     val_loss /= len(X_batches)\n",
    "\n",
    "#     train_loss_list.append(train_loss)\n",
    "#     val_loss_list  .append(val_loss)\n",
    "\n",
    "#     # progress\n",
    "#     now_time = str(datetime.datetime.now())[:-7]\n",
    "#     end_time = time.time()\n",
    "#     elapsed = end_time-start_time\n",
    "#     elapsed_times.append(elapsed)\n",
    "#     total = sum(elapsed_times)\n",
    "#     remaining = (epochs-i-1) * elapsed\n",
    "\n",
    "#     str_i = str(i+1).zfill(len(str(epochs)))\n",
    "#     progress = f'{now_time} | [{str_i}/{epochs}] {train_loss=:.4f}, {val_loss=:.4f}, {elapsed=:.1f}s, {total=:.1f}s, {remaining=:.1f}s'\n",
    "#     print(progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential\n",
    "# from tensorflow.keras.layers import Dense, BatchNormalization, ReLU, Dropout\n",
    "# from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# def f1_score(y_true, y_pred):\n",
    "#     y_pred = K.round(y_pred)\n",
    "    \n",
    "#     tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
    "\n",
    "#     precision = tp / (tp + fp + K.epsilon())\n",
    "#     recall = tp / (tp + fn + K.epsilon())\n",
    "    \n",
    "#     f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "#     return K.mean(f1)\n",
    "\n",
    "# # label -> onehot encoding\n",
    "# num_classes = len(np.unique(y_train))\n",
    "# y_train_onehot, y_test_onehot = label_onehot(y_train, num_classes), label_onehot(y_test, num_classes)\n",
    "\n",
    "# with tf.device('/cpu:0'):\n",
    "#     optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=64, input_dim=784, activation='relu'))\n",
    "#     model.add(Dense(units=32, input_dim=64, activation='relu'))\n",
    "#     model.add(Dense(units=10, activation='softmax'))\n",
    "#     model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',f1_score])\n",
    "#     model.summary()\n",
    "\n",
    "#     # model = Sequential()\n",
    "#     # model.add(Dense(units=64, input_dim=784)) # (1) Dense\n",
    "#     # model.add(BatchNormalization())           # (2) Batch Normalization\n",
    "#     # model.add(ReLU())                         # (3) Activation\n",
    "#     # model.add(Dropout(0.5))                   # (4) Dropout\n",
    "#     # model.add(Dense(units=32))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     # model.add(ReLU())\n",
    "#     # model.add(Dropout(0.5))\n",
    "#     # model.add(Dense(units=10, activation='softmax'))\n",
    "#     # model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy',f1_score])\n",
    "#     # model.summary()\n",
    "\n",
    "#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "#     # checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True)\n",
    "\n",
    "#     history = model.fit(\n",
    "#         tf.convert_to_tensor(X_train),\n",
    "#         tf.convert_to_tensor(y_train_onehot),\n",
    "#         validation_data=(tf.convert_to_tensor(X_test), tf.convert_to_tensor(y_test_onehot)),\n",
    "#         epochs=50,\n",
    "#         batch_size=64,\n",
    "#         callbacks=[early_stopping],\n",
    "#     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Pytorch (pytorch lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorch_lightning as pl\n",
    "# import torch\n",
    "# from torch import nn\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import torchmetrics\n",
    "# from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar, RichProgressBar\n",
    "\n",
    "# class SimpleModel(pl.LightningModule):\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         self.layer = nn.Sequential(\n",
    "#             nn.Linear(input_dim, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, output_dim)\n",
    "#         )\n",
    "#         self.criterion = nn.CrossEntropyLoss()\n",
    "#         self.optimizer = torch.optim.AdamW(self.parameters(), lr=0.001)\n",
    "#         self.f1 = torchmetrics.F1Score(task='multiclass', num_classes=output_dim, average='weighted')\n",
    "\n",
    "#         self.prog_bar = False\n",
    "#         self._callback_init()\n",
    "\n",
    "#     def _callback_init(self):\n",
    "#         self.callbacks = {\n",
    "#             'train': {'loss':[], 'f1':[]},\n",
    "#             'val'  : {'loss':[], 'f1':[]},\n",
    "#         }\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.layer(x)\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         return self.optimizer\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "\n",
    "#         # loss\n",
    "#         loss = self.criterion(y_hat, y)\n",
    "\n",
    "#         # f1 score\n",
    "#         preds = torch.argmax(y_hat, dim=1)\n",
    "#         f1 = self.f1(preds, y)\n",
    "        \n",
    "#         self.log(\"train_loss\", loss, prog_bar=self.prog_bar)\n",
    "#         self.log(\"train_f1\", f1, prog_bar=self.prog_bar)\n",
    "\n",
    "#         self.callbacks['train']['loss'].append(loss.item())\n",
    "#         self.callbacks['train']['f1'].append(f1.item())\n",
    "\n",
    "#         return loss\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         x, y = batch\n",
    "#         y_hat = self(x)\n",
    "\n",
    "#         # loss\n",
    "#         loss = self.criterion(y_hat, y)\n",
    "\n",
    "#         # f1 score\n",
    "#         preds = torch.argmax(y_hat, dim=1)\n",
    "#         f1 = self.f1(preds, y)\n",
    "        \n",
    "#         self.log(\"val_loss\", loss, prog_bar=self.prog_bar)\n",
    "#         self.log(\"val_f1\", f1, prog_bar=self.prog_bar)\n",
    "\n",
    "#         self.callbacks['val']['loss'].append(loss.item())\n",
    "#         self.callbacks['val']['f1'].append(f1.item())\n",
    "        \n",
    "#         return loss\n",
    "\n",
    "#     # def on_train_epoch_end(self):\n",
    "#     #     # 한 epoch이 끝나면 모든 배치의 손실을 출력\n",
    "#     #     train_loss = np.mean(self.callbacks['train']['loss'])\n",
    "#     #     val_loss   = np.mean(self.callbacks['val']['loss'])\n",
    "#     #     train_f1   = np.mean(self.callbacks['train']['f1'])\n",
    "#     #     val_f1     = np.mean(self.callbacks['val']['f1'])\n",
    "\n",
    "#     #     # Early Stopping 콜백을 확인\n",
    "#     #     for callback in self.trainer.callbacks:\n",
    "#     #         if isinstance(callback, pl.callbacks.EarlyStopping):\n",
    "#     #             mark = '*' if callback.wait_count==0 else ' '\n",
    "#     #             es_info = '' if callback.wait_count==0 else f', early_stopping: {callback.wait_count}/{callback.patience}'\n",
    "\n",
    "#     #     print(f'\\r{mark}{train_loss=:.4f}, {val_loss=:.4f}, {train_f1=:.4f}, {val_f1=:.4f}{es_info}')\n",
    "#     #     self._callback_init()\n",
    "\n",
    "# class CustomDataset(Dataset):\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = torch.tensor(X, dtype=torch.float32)\n",
    "#         self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.X)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.X[idx], self.y[idx]\n",
    "\n",
    "# # prepare dataset\n",
    "# train_dataset = CustomDataset(X_train, y_train)\n",
    "# val_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64)\n",
    "\n",
    "# # define model\n",
    "# model = SimpleModel(input_dim=X_train.shape[1], output_dim=len(np.unique(y_train)))\n",
    "\n",
    "# # callbacks\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',   # 모니터링할 메트릭 ('val_loss'를 모니터링)\n",
    "#     patience=5,           # 성능이 개선되지 않아도 몇 번의 epoch을 더 진행할지 설정\n",
    "#     verbose=False,        # 학습 중지 메시지를 출력하지 않음\n",
    "#     mode='min'            # 모니터링하는 값이 작을수록 좋은 경우 'min', 클수록 좋은 경우 'max'\n",
    "# )\n",
    "# # progress_bar = TQDMProgressBar(refresh_rate=10, leave=True)\n",
    "# # progress_bar = RichProgressBar(leave=True)\n",
    "\n",
    "# # traning\n",
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=50,\n",
    "#     callbacks=[early_stopping],\n",
    "#     default_root_dir='G:\\My Drive\\Storage\\Github\\hyuckjinkim\\.logs',\n",
    "# )\n",
    "# trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "# # tensorboard --logdir=\"G:\\My Drive\\Storage\\Github\\hyuckjinkim\\.logs\\lightning_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 초기값에 따른 활성화 값의 분포 확인 : 204p\n",
    "batch_size = 1024\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_sizes = [256,256,256,256,256]\n",
    "\n",
    "X_batches, y_batches = batch_split(X_train, y_train, batch_size, shuffle=True, random_state=42)\n",
    "\n",
    "i=0\n",
    "X_batch, y_batch = X_batches[i], y_batches[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Activation Function: Sigmoid')\n",
    "\n",
    "# (1) N(0,1) + sigmoid\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid', weight_init=None, weight_init_limit=1)\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(sigmoid)')\n",
    "plt.suptitle('(1) N(0,1)', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (2) N(0,0.01) + sigmoid\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid', weight_init=None, weight_init_limit=0.01)\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(sigmoid)')\n",
    "plt.suptitle('(2) N(0,0.01)', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (3) Xavier + sigmoid\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid', weight_init='Xavier')\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(sigmoid)')\n",
    "plt.suptitle('(3) Xavier', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (4) He + sigmoid\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='sigmoid', weight_init='He')\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(sigmoid)')\n",
    "plt.suptitle('(4) He', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('> Activation Function: ReLU')\n",
    "\n",
    "# (1) N(0,1) + ReLU\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='relu', weight_init=None, weight_init_limit=1)\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(relu)')\n",
    "plt.suptitle('(1) N(0,1)', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (2) N(0,0.01) + ReLU\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='relu', weight_init=None, weight_init_limit=0.01)\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(relu)')\n",
    "plt.suptitle('(2) N(0,0.01)', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (3) Xavier + ReLU\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='relu', weight_init='Xavier')\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(relu)')\n",
    "plt.suptitle('(3) Xavier', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (4) He + ReLU\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='relu', weight_init='He')\n",
    "_, activations = model.predict_numerical_gradient(X_batch, return_activations=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15,3))\n",
    "for i,activation in enumerate(activations):\n",
    "    fig.add_subplot(1, len(activations), i+1)\n",
    "    plt.hist(activation.flatten(), bins=30, range=(0,1))\n",
    "    plt.grid(alpha=0.5)\n",
    "    if i+1 == len(activations):\n",
    "        plt.title(f'{i+1}-layer\\n(softmax)')\n",
    "    else:\n",
    "        plt.title(f'{i+1}-layer\\n(relu)')\n",
    "plt.suptitle('(4) He', fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch normalization : 212p\n",
    "# -> https://github.com/WegraLee/deep-learning-from-scratch/blob/3e4f6368b17ee4159c820197426eefb0b1315a9b/common/layers.py#L114\n",
    "class BatchNormalization:\n",
    "    def __init__(self, gamma, beta, momentum=0.9, running_mean=None, running_var=None, eps=1e-6):\n",
    "        self.gamma = gamma\n",
    "        self.beta = beta\n",
    "        self.momentum = momentum\n",
    "        self.eps = eps\n",
    "\n",
    "        # 합성곱 계층은 4차원, 완전연결 계층은 2차원\n",
    "        self.input_shape = None\n",
    "\n",
    "        # 시험할 때 사용할 평균과 분산\n",
    "        self.running_mean = running_mean\n",
    "        self.running_var = running_var\n",
    "\n",
    "        # backward 시에 사용할 중간 데이터\n",
    "        self.batch_size = None\n",
    "        self.xc = None\n",
    "        self.std = None\n",
    "        self.dgamma = None\n",
    "        self.dbeta = None\n",
    "\n",
    "    def __reshape(self, x):\n",
    "        if x.ndim != 2:\n",
    "            # (batch_size, channel, height, width)\n",
    "            # - channel: RGB(3), Gray(1)\n",
    "            # - height, width : height, width of image (64x64 or 256x256)\n",
    "            N, C, H, W = x.shape \n",
    "            x = x.reshape(N, -1)\n",
    "        return x\n",
    "\n",
    "    def __forward(self, x, train_flg):\n",
    "        if self.running_mean is None:\n",
    "            # N: batch size, D: input size(feature size)\n",
    "            N, D = x.shape\n",
    "            self.running_mean = np.zeros(D)\n",
    "            self.running_var = np.zeros(D)\n",
    "\n",
    "        if train_flg:\n",
    "            mu = np.mean(x, axis=0)\n",
    "            xc = x - mu                  # centered\n",
    "            var = np.mean(xc**2, axis=0) # (mu, xc, xn)에 대한 정보를 얻기위해서 np.std를 사용하지 않고 분산공식으로 계산\n",
    "            std = np.sqrt(var + self.eps)\n",
    "            xn = xc / std                # normalized\n",
    "\n",
    "            self.batch_size = x.shape[0]\n",
    "            self.xc = xc\n",
    "            self.xn = xn\n",
    "            self.std = std\n",
    "\n",
    "            self.running_mean = self.momentum * self.running_mean + (1-self.momentum) * mu\n",
    "            self.running_var  = self.momentum * self.running_var  + (1-self.momentum) * var\n",
    "        else:\n",
    "            xc = x - self.running_mean\n",
    "            xn = xc / np.sqrt(self.running_var + self.eps)\n",
    "\n",
    "        # (x-mu)/std의 inverse transform = scale * normalized + shift\n",
    "        out = self.gamma * xn + self.beta\n",
    "\n",
    "        return out\n",
    "\n",
    "    def forward(self, x, train_flg):\n",
    "        self.input_shape = x.shape\n",
    "\n",
    "        x = self.__reshape(x)\n",
    "        out = self.__forward(x, train_flg)\n",
    "        out = out.reshape(*self.input_shape)\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def __backward(self, dout):\n",
    "        # backward by Chain Rule\n",
    "        # - (역전파 그림 참조) https://velog.io/@clayryu328/밑바닥부터-시작하는-딥러닝-13-배치정규화\n",
    "        #   -> (4번, 6번에서 참조하면 좋음)\n",
    "\n",
    "        # (1) y = gamma * xn + beta\n",
    "        dbeta = np.sum(dout, axis=0)            #-> dL/dbeta = dL/dy * dy/dbeta = dL/dy * 1 = dL/dy\n",
    "        dgamma = np.sum(dout * self.xn, axis=0) #-> dL/dgamma = dL/dy * dy/dgamma = dL/dy * xn\n",
    "        # print(dgamma[:3])\n",
    "        dxn = dout * self.gamma                 #-> dL/dxn = dL/dy * dy/dxn = dL/dy * gamma\n",
    "\n",
    "        # (2) xn = xc / std\n",
    "        dstd = - np.sum(dxn * self.xc / (self.std**2), axis=0) #-> dL/dstd = dL/dxn * dxn/dstd = dL/dxn * xc * -(1/std)**2\n",
    "        \n",
    "        # (3) std = var**(1/2)\n",
    "        dvar = dstd * 0.5 / self.std #dL/dvar = dL/dstd * dstd/dvar = dL/dstd * (1/2) * var**(-1/2) = dL/dstd * (1/2) * (1/std)\n",
    "\n",
    "        # (4) var = 1/n * sum(xc**2)\n",
    "        #  두개의 xc가 합류를 하기 때문에 더해준다\n",
    "        #  (4-1) xn = xc / std           -> dL/dxc = dL/dxn * dxn/dxc = dL/dxn * (1/std)\n",
    "        #  (4-2) var = 1/n * sum(xc**2)  -> dL/dxc = dL/dvar * dvar/dxc = dL/dvar * (2/n) * xc\n",
    "        dxc = (dxn / self.std) + (dvar * (2.0/self.batch_size) * self.xc)\n",
    "\n",
    "        # (5) xc = x - mu\n",
    "        dmu = np.sum(dxc, axis=0) # dL/dmu = dL/dxc * dxc/dmu = dL/dxc * 1 = dL/dxc\n",
    "\n",
    "        # (6) xc와 mu의 평균이 합류를 하기 때문에 더해준다\n",
    "        #  (6-1) xc = x - mu        -> dL/dx = dL/dxc * dxc/dx = dL/dxc * 1 = dL/dxc\n",
    "        #  (6-2) mu = 1/N * sum(x)  -> dL/dx = dL/dmu * dmu/dx = dL/dmu * (1/N)\n",
    "        dx = dxc - (dmu / self.batch_size)\n",
    "\n",
    "        self.dgamma = dgamma\n",
    "        self.dbeta = dbeta\n",
    "\n",
    "        return dx\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout = self.__reshape(dout)\n",
    "        dout = self.__backward(dout)\n",
    "        dout = dout.reshape(*self.input_shape)\n",
    "        return dout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropout : 219p\n",
    "class Dropout:\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x, train_flg=True):\n",
    "        if train_flg:\n",
    "            self.mask = np.random.rand(*x.shape) > self.dropout_rate # dropout_rate보다 높은건(삭제안할거) True, 낮은건(삭제할거) False\n",
    "            return x * self.mask # dropout_rate보다 높은건 그대로 보내고, 낮은건 0으로 없앰\n",
    "        else:\n",
    "            return x * (1.0 - self.dropout_rate) # 모든 뉴런을 활성화하되, 훈련 시의 비활성화된 뉴런을 고려하여 값을 \"보정\"해야함. 이때, 1 - dropout_rate로 보정하는 것이 일반적임.\n",
    "                                                 # dropout_rate 만큼 감소되었다고 치고 보정함.\n",
    "\n",
    "    def backward(self, dout):\n",
    "        return dout * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "from collections import OrderedDict\n",
    "from copy import deepcopy\n",
    "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
    "\n",
    "class MultiLayerNet:\n",
    "    def __init__(self, input_size, output_size, hidden_sizes, activation=None, weight_init='auto',\n",
    "                 dropout=None, weight_decay_lambda=0, random_state=42, batchnorm=False):\n",
    "        assert str(activation).lower() in ['none','sigmoid','relu','leakyrelu'], \"activation must be one of ['sigmoid','relu','leakyrelu'] or None.\"\n",
    "        assert str(weight_init).lower() in ['auto','xavier','he','none'], \"weight_init must be one of ['auto','Xavier','He',None].\"\n",
    "        assert (dropout is None) or (0<=dropout<=1), \"dropout must be between 0 and 1 or None.\"\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.dropout = dropout\n",
    "        self.weight_decay_lambda = weight_decay_lambda\n",
    "        self.batchnorm = batchnorm\n",
    "\n",
    "        self.node_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        self.n_layers = len(self.node_sizes)-1\n",
    "\n",
    "        # activation\n",
    "        self.activation = self.__activation_init(activation)\n",
    "\n",
    "        # weight init\n",
    "        self.__weight_init(weight_init, activation, random_state)\n",
    "\n",
    "        # define layers\n",
    "        self.__layers_init()\n",
    "\n",
    "    def __activation_init(self, activation):\n",
    "        if str(activation).lower()=='none':\n",
    "            activation = 'sigmoid'\n",
    "        else:\n",
    "            activation = str(activation).lower()\n",
    "\n",
    "        activation_dict = {\n",
    "            'sigmoid' : Sigmoid(),\n",
    "            'relu' : ReLU(),\n",
    "            'leakyrelu' : LeakyReLU(),\n",
    "        }\n",
    "        return activation_dict.get(activation)\n",
    "\n",
    "    # > 책 205p\n",
    "    # > torch 공식문서\n",
    "    #   - nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html\n",
    "    #   - nn.Conv2d: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n",
    "    def __weight_init(self, method, activation, seed=42):\n",
    "        method = str(method).lower()\n",
    "        if method == 'auto':\n",
    "            if activation == 'sigmoid':\n",
    "                method = 'xavier'\n",
    "            elif activation in ['relu','leakyrelu']:\n",
    "                method = 'he'\n",
    "            else:\n",
    "                method = 'none'\n",
    "\n",
    "        np.random.seed(seed)\n",
    "        self.params = {'weights': OrderedDict(), 'biases': OrderedDict(), 'gamma': OrderedDict(), 'beta': OrderedDict()}\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            input_dim = self.node_sizes[i]\n",
    "            output_dim = self.node_sizes[i+1]\n",
    "\n",
    "            if method == 'none':\n",
    "                scale = 0.01\n",
    "            elif method == 'xavier':\n",
    "                scale = np.sqrt(1 / input_dim)   # Xavier initialization\n",
    "            elif method == 'he':\n",
    "                scale = np.sqrt(2 / input_dim)   # He initialization\n",
    "            else:\n",
    "                raise ValueError\n",
    "\n",
    "            self.params['weights'][f'w{i+1}'] = scale * np.random.randn(input_dim, output_dim)\n",
    "            self.params['biases'] [f'b{i+1}'] = scale * np.random.randn(output_dim)\n",
    "\n",
    "    def __layers_init(self):\n",
    "        self.layers = OrderedDict()\n",
    "        for i in range(self.n_layers):\n",
    "            is_last = (i+1 == self.n_layers)\n",
    "            input_dim = self.node_sizes[i]\n",
    "            output_dim = self.node_sizes[i+1]\n",
    "\n",
    "            # Affine\n",
    "            W = self.params['weights'][f'w{i+1}']\n",
    "            b = self.params['biases'] [f'b{i+1}']\n",
    "            self.layers[f'Affine{i+1}'] = Affine(W,b)\n",
    "\n",
    "            # Batch Normalization\n",
    "            if (self.batchnorm) and (not is_last):\n",
    "                self.params['gamma'][f'g{i+1}'] = np.ones(output_dim)\n",
    "                self.params['beta'] [f'b{i+1}'] = np.zeros(output_dim)\n",
    "                self.layers[f'BatchNorm{i+1}'] = BatchNormalization(self.params['gamma'][f'g{i+1}'], self.params['beta'] [f'b{i+1}'])\n",
    "\n",
    "            # Activation\n",
    "            if not is_last:\n",
    "                self.layers[f'Activation{i+1}'] = deepcopy(self.activation)\n",
    "\n",
    "            # Dropout\n",
    "            if (self.dropout is not None) and (not is_last):\n",
    "                self.layers[f'Dropout{i+1}'] = Dropout(self.dropout)\n",
    "\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "\n",
    "    def predict(self, x, train_flg=False):\n",
    "        for key, layer in self.layers.items():\n",
    "            if (\"dropout\" in key.lower()) or (\"batchnorm\" in key.lower()):\n",
    "                x = layer.forward(x, train_flg)\n",
    "            else:\n",
    "                x = layer.forward(x)\n",
    "        return x\n",
    "\n",
    "    def loss(self, x, y_true, train_flg=False):\n",
    "        logits = self.predict(x, train_flg)\n",
    "\n",
    "        # weight decay (L2 정규화) : 217p\n",
    "        # -> https://github.com/WegraLee/deep-learning-from-scratch/blob/3e4f6368b17ee4159c820197426eefb0b1315a9b/common/multi_layer_net_extend.py#L91\n",
    "        # -> 큰 가중치에 대해서 그에 상응하는 큰 페널티를 부과하여 오버피팅을 억제\n",
    "        weight_decay = 0\n",
    "        for i in range(self.n_layers):\n",
    "            W = self.params['weights'][f'w{i+1}']\n",
    "            weight_decay += 0.5 * self.weight_decay_lambda * np.sum(W**2)\n",
    "\n",
    "        loss = self.last_layer.forward(y=logits, t=y_true) + weight_decay\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def accuracy(self, x, y_true):\n",
    "        logits = self.predict(x, train_flg=False)\n",
    "        y_pred = np.argmax(logits, axis=1)\n",
    "        if y_true.ndim != 1:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "\n",
    "        acc = np.sum(y_true==y_pred) / float(x.shape[0])\n",
    "        \n",
    "        return acc\n",
    "\n",
    "    def gradient(self, x, y_true):\n",
    "        # 순전파\n",
    "        self.loss(x, y_true, train_flg=True)\n",
    "\n",
    "        # 역전파\n",
    "        dout = self.last_layer.backward()\n",
    "        for key, layer in reversed(self.layers.items()):\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # params 저장\n",
    "        params = {'weights': OrderedDict(), 'biases': OrderedDict(), 'gamma': OrderedDict(), 'beta': OrderedDict()}\n",
    "        for i in range(self.n_layers):\n",
    "            is_last = (i+1 == self.n_layers)\n",
    "            params['weights'][f'w{i+1}'] = self.layers[f'Affine{i+1}'].dW + (self.weight_decay_lambda * self.params['weights'][f'w{i+1}'])\n",
    "            params['biases'] [f'b{i+1}'] = self.layers[f'Affine{i+1}'].db\n",
    "            if self.batchnorm and (not is_last):\n",
    "                params['gamma'][f'g{i+1}'] = self.layers[f'BatchNorm{i+1}'].dgamma\n",
    "                params['beta'] [f'b{i+1}'] = self.layers[f'BatchNorm{i+1}'].dbeta\n",
    "\n",
    "        return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_sizes = [8]\n",
    "\n",
    "model = MultiLayerNet(input_size, output_size, hidden_sizes, activation='LeakyReLU', dropout=0.5, weight_decay_lambda=1e-5, batchnorm=True)\n",
    "optimizer = SGD(lr=lr) # SGD, Momentum, AdaGrad, RMSProp, Adam\n",
    "\n",
    "X_batches, y_batches = batch_split(X_train, y_train, batch_size, shuffle=True, random_state=42)\n",
    "\n",
    "elapsed_times = []\n",
    "for i in range(epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    train_acc = 0\n",
    "    val_acc = 0\n",
    "\n",
    "    # get mini-batch (실제로는 모든 6만개의 데이터에 대해서 모두 가중치 업데이트를 해야 1개의 epoch로 쳐줌)\n",
    "    pbar_batch = tqdm(zip(X_batches, y_batches), total=len(X_batches))\n",
    "    for X_batch, y_batch in pbar_batch:\n",
    "        # calculate gradients\n",
    "        grads = model.gradient(X_batch, y_batch)\n",
    "\n",
    "        # gradient descent\n",
    "        model.params = optimizer.update(model.params, grads)\n",
    "\n",
    "        for k in range(model.n_layers):\n",
    "            W = model.params['weights'][f'w{k+1}']\n",
    "            b = model.params['biases'] [f'b{k+1}']\n",
    "            model.layers[f'Affine{k+1}'] = Affine(W, b)\n",
    "\n",
    "        # loss\n",
    "        train_loss += model.loss(X_batch, y_batch, train_flg=False)\n",
    "        val_loss   += model.loss(X_test , y_test , train_flg=False)\n",
    "\n",
    "        # acc\n",
    "        train_acc += model.accuracy(X_batch, y_batch)\n",
    "        val_acc   += model.accuracy(X_test , y_test)\n",
    "\n",
    "    # calculate loss\n",
    "    train_loss /= len(X_batches)\n",
    "    val_loss /= len(X_batches)\n",
    "\n",
    "    # calculate accuracy\n",
    "    train_acc /= len(X_batches)\n",
    "    val_acc /= len(X_batches)\n",
    "\n",
    "    # progress\n",
    "    now_time = str(datetime.datetime.now())[:-7]\n",
    "    end_time = time.time()\n",
    "    elapsed = end_time-start_time\n",
    "    elapsed_times.append(elapsed)\n",
    "    total = sum(elapsed_times)\n",
    "    remaining = (epochs-i-1) * elapsed\n",
    "\n",
    "    str_i = str(i+1).zfill(len(str(epochs)))\n",
    "    progress = f'{now_time} | [{str_i}/{epochs}] {train_loss=:.4f}, {val_loss=:.4f}, {train_acc=:.4f}, {val_acc=:.4f}, {elapsed=:.1f}s, {total=:.1f}s, {remaining=:.1f}s'\n",
    "    print(progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
